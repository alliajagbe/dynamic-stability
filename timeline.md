# Timeline: Empirical Evaluation and Comparison of Reinforcement Learning Algorithms

### Weeks 1-2: Selection and Literature Review
  - [x] We will identify multiple algorithms(Qlearning,Deep QNetwork,Proximal Policy Optimization (PPO), etc) suitable for dynamic stabilization from the provided list or additional credible sources. 
  - [x] Conducting a thorough literature review on existing databases around RL modules used for stabilization. 
  - [x] Finally all three of us will identify an illustrative scenario for subsequent simulation - CartPole from OpenAI Gym.

### Weeks 3-4: Implementation Setup
  - [x] We will then set up a development environment with the necessary tools and libraries for reinforcement learning. Beginning the development and coding of these algorithms.
  - [x] Explore the implementation of advanced algorithms like PPO, QLearning etc, comparing their theoretical and approximately practical advantages.
  - [x] All of us plan to utilize the OpenAI Gym environment to simulate basic stabilization tasks, fine-tuning the algorithms to achieve the expected performance.

### Weeks 5-6: Model Development and Preliminary Testing
  - [x] We will define evaluation metrics such as convergence speed, stability, and adaptability to ensure a fair comparison.
  - [x] We plan to use Weights & Biases (Wandb) for tracking experiments, comparing model performances, and logging initial results.
  - [x] Compare the performance of various algorithms (Q-learning, PPO, etc.) using predefined metrics for stability, efficiency, and adaptability.

### Weeks 7-8: In-depth Testing, Evaluation, and Optimization
  - [x] Optimize models based on testing feedback.
  - [x] Utilize Wandb extensively for experiment tracking, result comparison, and identifying best-performing models.

### Weeks 9-10: Extensive Testing, Simulation, and Documentation
  - [x] Conduct final evaluations, comparing the models against all predefined metrics and documenting the efficiency of different algorithms.
  - [x] Document our methodology, results, and insights in a detailed report that encapsulates our findings and contributions to dynamic stability in balancing systems similar to the CartPole Dynamics.
